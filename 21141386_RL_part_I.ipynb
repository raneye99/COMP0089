{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raneye99/COMP0089/blob/main/21141386_RL_part_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL coursework, part I (20 pts total)\n",
        "---\n",
        "\n",
        "**SN:** 21141386\n",
        "\n",
        "---\n",
        "\n",
        "**Due date:** *22nd March, 2022,*\n",
        "\n",
        "---\n",
        "\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part1.ipynb`** before the deadline above, where `<studentnumber>` is your student number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "source": [
        "**Context**\n",
        "\n",
        "In this assignment, we will take a first look at learning decisions from data.  For this, we will use the multi-armed bandit framework.\n",
        "\n",
        "**Background reading**\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 1 to 6\n",
        "* Lecture slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "**Overview of this assignment**\n",
        "\n",
        "A) You will use Python to implement several bandit algorithms.\n",
        "\n",
        "B) You will then run these algorithms on a multi-armed Bernoulli bandit problem, and answer question about their empirical performance.\n",
        "\n",
        "C) You will then be asked to reason about the behaviour of different algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run each of the cells below, until you reach the next section **Basic Agents**. You do not have to read or understand the code in the **Setup** section.  After running the cells, feel free to fold away the **Setup** section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzYtxi8Wh5SJ"
      },
      "outputs": [],
      "source": [
        "# Import Useful Libraries\n",
        "\n",
        "import collections\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-notebook')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP97bVN3NuG8"
      },
      "outputs": [],
      "source": [
        "class BernoulliBandit(object):\n",
        "  \"\"\"A stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "\n",
        "  def __init__(self, success_probabilities, success_reward=1., fail_reward=0.):\n",
        "    \"\"\"Constructor of a stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "\n",
        "    ps = np.array(success_probabilities)\n",
        "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or np.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A reward sampled according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "    return reward\n",
        "\n",
        "  def regret(self, action):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max() - self._values[action]\n",
        "\n",
        "  def optimal_value(self):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYxNiGcRxbd0"
      },
      "outputs": [],
      "source": [
        "class NonStationaryBandit(object):\n",
        "  \"\"\"A non-stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "\n",
        "  def __init__(self, success_probabilities,\n",
        "               success_reward=1., fail_reward=0., change_point=800,\n",
        "               change_is_good=True):\n",
        "    \"\"\"Constructor of a non-stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "      change_point: The number of steps before the rewards change.\n",
        "      change_is_good: Whether the rewards go up (if True), or flip (if False).\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "    self._change_point = change_point\n",
        "    self._change_is_good = change_is_good\n",
        "    self._number_of_steps_so_far = 0\n",
        "\n",
        "    ps = np.array(success_probabilities)\n",
        "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or np.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A reward sampled according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    self._number_of_steps_so_far += 1\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "    \n",
        "    if self._number_of_steps_so_far == self._change_point:\n",
        "      # After some number of steps, the rewards are inverted\n",
        "      #\n",
        "      #  ``The past was alterable. The past never had been altered. Oceania was\n",
        "      #    at war with Eastasia. Oceania had always been at war with Eastasia.``\n",
        "      #            - 1984, Orwell (1949).\n",
        "      reward_dif = (self._s - self._f)\n",
        "      if self._change_is_good:\n",
        "        self._f = self._s + reward_dif\n",
        "      else:\n",
        "        self._s -= reward_dif\n",
        "        self._f += reward_dif\n",
        "      \n",
        "      # Recompute expected values when the rewards change\n",
        "      ps = np.array(self._probs)\n",
        "      self._values = ps * self._s + (1 - ps) * self._f\n",
        "\n",
        "    return reward\n",
        "  \n",
        "  def regret(self, action):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max() - self._values[action]\n",
        "  \n",
        "  def optimal_value(self):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU7KGFJ0DN-H"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\n",
        "def smooth(array, smoothing_horizon=100., initial_value=0.):\n",
        "  \"\"\"Smoothing function for plotting.\"\"\"\n",
        "  smoothed_array = []\n",
        "  value = initial_value\n",
        "  b = 1./smoothing_horizon\n",
        "  m = 1.\n",
        "  for x in array:\n",
        "    m *= 1. - b\n",
        "    lr = b/(1 - m)\n",
        "    value += lr*(x - value)\n",
        "    smoothed_array.append(value)\n",
        "  return np.array(smoothed_array)\n",
        "\n",
        "def plot(algs, plot_data, repetitions=30):\n",
        "  \"\"\"Plot results of a bandit experiment.\"\"\"\n",
        "  algs_per_row = 4\n",
        "  n_algs = len(algs)\n",
        "  n_rows = (n_algs - 2)//algs_per_row + 1\n",
        "  fig = plt.figure(figsize=(10, 4*n_rows))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.35)\n",
        "  clrs = ['#000000', '#00bb88', '#0033ff', '#aa3399', '#ff6600']\n",
        "  lss = ['--', '-', '-', '-', '-']\n",
        "  for i, p in enumerate(plot_data):\n",
        "    for c in range(n_rows):\n",
        "      ax = fig.add_subplot(n_rows, len(plot_data), i + 1 + c*len(plot_data))\n",
        "      ax.grid(0)\n",
        "\n",
        "      current_algs = [algs[0]] + algs[c*algs_per_row + 1:(c + 1)*algs_per_row + 1]\n",
        "      for alg, clr, ls in zip(current_algs, clrs, lss):\n",
        "        data = p.data[alg.name]\n",
        "        m = smooth(np.mean(data, axis=0))\n",
        "        s = np.std(smooth(data.T).T, axis=0)/np.sqrt(repetitions)\n",
        "        if p.log_plot:\n",
        "          line = plt.semilogy(m, alpha=0.7, label=alg.name,\n",
        "                              color=clr, ls=ls, lw=3)[0]\n",
        "        else:\n",
        "          line = plt.plot(m, alpha=0.7, label=alg.name,\n",
        "                          color=clr, ls=ls, lw=3)[0]\n",
        "          plt.fill_between(range(len(m)), m + s, m - s,\n",
        "                           color=line.get_color(), alpha=0.2)\n",
        "      if p.opt_values is not None:\n",
        "        plt.plot(p.opt_values[current_algs[0].name][0], ':', alpha=0.5,\n",
        "                 label='optimal')\n",
        "\n",
        "      ax.set_facecolor('white')\n",
        "      ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",\n",
        "                     labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
        "      ax.spines[\"top\"].set_visible(False)\n",
        "      ax.spines[\"bottom\"].set(visible=True, color='black', lw=1)\n",
        "      ax.spines[\"right\"].set_visible(False)\n",
        "      ax.spines[\"left\"].set(visible=True, color='black', lw=1)\n",
        "      ax.get_xaxis().tick_bottom()\n",
        "      ax.get_yaxis().tick_left()\n",
        "\n",
        "      data = np.array([smooth(np.mean(d, axis=0)) for d in p.data.values()])\n",
        "      \n",
        "      if p.log_plot:\n",
        "        start, end = calculate_lims(data, p.log_plot)\n",
        "        start = np.floor(np.log10(start))\n",
        "        end = np.ceil(np.log10(end))\n",
        "        ticks = [_*10**__\n",
        "                 for _ in [1., 2., 3., 5.]\n",
        "                 for __ in [-2., -1., 0.]]\n",
        "        labels = [r'${:1.2f}$'.format(_*10** __)\n",
        "                  for _ in [1, 2, 3, 5]\n",
        "                  for __ in [-2, -1, 0]]\n",
        "        plt.yticks(ticks, labels)\n",
        "      plt.ylim(calculate_lims(data, p.log_plot))\n",
        "      plt.locator_params(axis='x', nbins=4)\n",
        "      \n",
        "      plt.title(p.title)\n",
        "      if i == len(plot_data) - 1:\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n",
        "def run_experiment(bandit_constructor, algs, repetitions, number_of_steps):\n",
        "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
        "  reward_dict = {}\n",
        "  regret_dict = {}\n",
        "  optimal_value_dict = {}\n",
        "\n",
        "  for alg in algs:\n",
        "    reward_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    regret_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    optimal_value_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "\n",
        "    for _rep in range(repetitions):\n",
        "      bandit = bandit_constructor()\n",
        "      alg.reset()\n",
        "\n",
        "      action = None\n",
        "      reward = None\n",
        "      for _step in range(number_of_steps):\n",
        "        action = alg.step(action, reward)\n",
        "        reward = bandit.step(action)\n",
        "        regret = bandit.regret(action)\n",
        "        optimal_value = bandit.optimal_value()\n",
        "\n",
        "        reward_dict[alg.name][_rep, _step] = reward\n",
        "        regret_dict[alg.name][_rep, _step] = regret\n",
        "        optimal_value_dict[alg.name][_rep, _step] = optimal_value\n",
        "\n",
        "  return reward_dict, regret_dict, optimal_value_dict\n",
        "\n",
        "\n",
        "def train_agents(agents, number_of_arms, number_of_steps, repetitions=100,\n",
        "                 success_reward=1., fail_reward=0.,\n",
        "                 bandit_class=BernoulliBandit):\n",
        "\n",
        "  success_probabilities = np.arange(0.3, 0.7 + 1e-6, 0.4/(number_of_arms - 1))\n",
        "\n",
        "  bandit_constructor = partial(bandit_class,\n",
        "                               success_probabilities=success_probabilities,\n",
        "                               success_reward=success_reward,\n",
        "                               fail_reward=fail_reward)\n",
        "  rewards, regrets, opt_values = run_experiment(\n",
        "      bandit_constructor, agents, repetitions, number_of_steps)\n",
        "\n",
        "  smoothed_rewards = {}\n",
        "  for agent, rs in rewards.items():\n",
        "    smoothed_rewards[agent] = np.array(rs)\n",
        "\n",
        "  PlotData = collections.namedtuple('PlotData',\n",
        "                                    ['title', 'data', 'opt_values', 'log_plot'])\n",
        "  total_regrets = dict([(k, np.cumsum(v, axis=1)) for k, v in regrets.items()])\n",
        "  plot_data = [\n",
        "      PlotData(title='Smoothed rewards', data=smoothed_rewards,\n",
        "               opt_values=opt_values, log_plot=False),\n",
        "      PlotData(title='Current Regret', data=regrets, opt_values=None,\n",
        "               log_plot=True),\n",
        "      PlotData(title='Total Regret', data=total_regrets, opt_values=None,\n",
        "               log_plot=False),\n",
        "  ]\n",
        "\n",
        "  plot(agents, plot_data, repetitions)\n",
        "\n",
        "def calculate_lims(data, log_plot=False):\n",
        "  y_min = np.min(data)\n",
        "  y_max = np.max(data)\n",
        "  diff = y_max - y_min\n",
        "  if log_plot:\n",
        "    y_min = 0.9*y_min\n",
        "    y_max = 1.1*y_max\n",
        "  else:\n",
        "    y_min = y_min - 0.05*diff\n",
        "    y_max = y_max + 0.05*diff\n",
        "  return y_min, y_max\n",
        "\n",
        "def argmax(array):\n",
        "  \"\"\"Returns the maximal element, breaking ties randomly.\"\"\"\n",
        "  return np.random.choice(np.flatnonzero(array == array.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "# A) Agent implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBHsuFyapu5r"
      },
      "source": [
        "\n",
        "All agents should be in pure Python/NumPy.\n",
        "\n",
        "You cannot use any AutoDiff packages (Jax, TF, PyTorch, etc.)\n",
        "\n",
        "Each agent, should implement the following methods:\n",
        "\n",
        "**`step(self, previous_action, reward)`:**\n",
        "\n",
        "Should update the statistics by updating the value for the previous_action towards the observed reward.\n",
        "\n",
        "(Note: make sure this can handle the case that previous_action=None, in which case no statistics should be updated.)\n",
        "\n",
        "(Hint: you can split this into two steps: 1. update values, 2. get new action.  Make sure you update the values before selecting a new action.)\n",
        "\n",
        "**`reset(self)`:**\n",
        "\n",
        "Resets statistics (should be equivalent to constructing a new agent from scratch).\n",
        "\n",
        "Make sure that the initial values (after a reset) are all zero.\n",
        "\n",
        "**`__init__(self, name, number_of_arms, *args)`:**\n",
        "\n",
        "The `__init__` should take at least an argument `number_of_arms`, and (potentially) agent specific args."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwUUBXgQ2MCk"
      },
      "source": [
        "## Example agent\n",
        "\n",
        "The following code block contains an example random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPYlY9M22JOI"
      },
      "outputs": [],
      "source": [
        "class Random(object):\n",
        "  \"\"\"A random agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms', uniformly at\n",
        "  random. The 'previous_action' argument of 'step' is ignored.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, number_of_arms):\n",
        "    \"\"\"Initialise the agent.\n",
        "    \n",
        "    Sets the name to `random`, and stores the number of arms. (In multi-armed\n",
        "    bandits `arm` is just another word for `action`.)\n",
        "    \"\"\"\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self.name = name\n",
        "\n",
        "  def step(self, unused_previous_action, unused_reward):\n",
        "    \"\"\"Returns a random action.\n",
        "    \n",
        "    The inputs are ignored, but this function still requires an action and a\n",
        "    reward, to have the same interface as other agents who may use these inputs\n",
        "    to learn.\n",
        "    \"\"\"\n",
        "    return np.random.randint(self._number_of_arms)\n",
        "\n",
        "  def reset(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDTyvlZsvSQq"
      },
      "source": [
        "\n",
        "## Q1 [2 pts]\n",
        "Implement a UCB agent.\n",
        "\n",
        "The `bonus_multiplier` is the parameter $c$ from the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM5NtZ3Q2X0F"
      },
      "outputs": [],
      "source": [
        "class UCB(object):\n",
        "  def __init__(self, name, number_of_arms, bonus_multiplier):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._bonus_multiplier = bonus_multiplier\n",
        "    self.name = name\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    #if no previous action pull a random arm\n",
        "    if previous_action == None:\n",
        "      action = np.random.randint(self._number_of_arms)\n",
        "    else:\n",
        "      #update values:\n",
        "\n",
        "      #time step\n",
        "      self.t += 1\n",
        "      #number of times previous action taken (count)\n",
        "      self.N[previous_action] += 1\n",
        "      #action value\n",
        "      self.Q[previous_action] += (reward - self.Q[previous_action])/self.N[previous_action]\n",
        "      \n",
        "      #find new action\n",
        "\n",
        "      #calc upper confidence bound for new action\n",
        "      U = np.sqrt(np.log(self.t)/(self.N+1))\n",
        "      #argmax\n",
        "      action = np.argmax(self.Q + self._bonus_multiplier*U)\n",
        "      \n",
        "    # ...\n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    #return values to 0 timestep\n",
        "    self.t = 0\n",
        "    self.Q = np.zeros(self._number_of_arms)\n",
        "    self.N = np.zeros(self._number_of_arms)\n",
        "    # ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqJxDegZtqXN"
      },
      "source": [
        "## Q2 [1 pt]\n",
        "Implement an $\\epsilon$-greedy agent.\n",
        "\n",
        "This agent should be able to support time-changing $\\epsilon$ schedules.\n",
        "\n",
        "Thus, your agent should accept both constants and callables as constructor argument `epsilon`; callables are used to decay the $\\epsilon$ parameter over time, for instance according to a polynomial schedule: $\\epsilon_t = t^{-\\eta}$ with $\\eta \\in [0, 1]$).\n",
        "\n",
        "\n",
        "If multiple actions have the same value, ties should be broken randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_1pB2p7146i"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedy(object):\n",
        "  \"\"\"An epsilon-greedy agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms'; with probability\n",
        "  `(1-epsilon)` it chooses the action with the highest estimated value, while\n",
        "  with probability `epsilon` it samples an action uniformly at random.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, number_of_arms, epsilon=0.1):\n",
        "    self.name = name\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._epsilon = epsilon\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    \"\"\"Update the learnt statistics and return an action.\n",
        "\n",
        "    A single call to step uses the provided reward to update the value of the\n",
        "    taken action (which is also provided as an input), and returns an action.\n",
        "    The action is either uniformly random (with probability epsilon), or greedy\n",
        "    (with probability 1 - epsilon).\n",
        "\n",
        "    If the input action is None (typically on the first call to step), then no\n",
        "    statistics are updated, but an action is still returned.\n",
        "    \"\"\"\n",
        "\n",
        "    if previous_action == None:\n",
        "      action = np.random.randint(self._number_of_arms)\n",
        "    else:\n",
        "      #update values\n",
        "      self.t +=1\n",
        "      #number of times previous action taken (count)\n",
        "      self.N[previous_action] += 1\n",
        "      #action value\n",
        "      self.Q[previous_action] += (reward - self.Q[previous_action])/self.N[previous_action]\n",
        "\n",
        "      #update action\n",
        "      if callable(self._epsilon):\n",
        "        ep = self._epsilon(self.t)\n",
        "      else:\n",
        "        ep = self._epsilon\n",
        "      \n",
        "      if np.random.random() <= ep:\n",
        "        action = np.random.randint(self._number_of_arms)\n",
        "      else:\n",
        "        # action = np.argmax(self.Q)\n",
        "        action = np.random.choice(np.where(self.Q==self.Q.max())[0])\n",
        "      \n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    self.Q = np.zeros(self._number_of_arms)\n",
        "    self.N = np.zeros(self._number_of_arms)\n",
        "    self.t = 0\n",
        "    # ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKI7uNjI1Ym"
      },
      "source": [
        "## Q3 [2 pts]\n",
        "Implement a REINFORCE agent.\n",
        "\n",
        "While `softmax` distributions are a common parametrization for policies over discrete action-spaces, they are not the only choice. In this exercise we ask you to implement REINFORCE with the `square-max` policy parameterization. With this parametrisation the probabilities depend on the action preferences $p(\\cdot)$ according to the expression:\n",
        "\n",
        "$$\\pi(a) = \\frac{p(a)^2}{\\sum_b p(b)^2}\\,.$$\n",
        "\n",
        "Implement a REINFORCE policy-gradient method for updating the preferences under this policy distribution. The action preferences are stored separately, so that for each action $a$ the preference $p(a)$ is a single value that you directly update.\n",
        "\n",
        "The agent should be able to use a baseline or not (as defined in the constructor). The `step_size` parameter $\\alpha$ used to update the policy must also be configurable in the constructor.\n",
        "\n",
        "The baseline should track the average reward so far, using the same `step_size` used to update the policy.\n",
        "\n",
        "The `step_size` and whether or not a baseline is used are defined in the constructor by feeding additional arguments in place of `...` below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqcC-OZq9bP7"
      },
      "outputs": [],
      "source": [
        "class REINFORCE(object):\n",
        "  def __init__(self, name, number_of_arms, step_size=0.1, baseline=False):\n",
        "    self.name = name\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._step_size = step_size\n",
        "    self._baseline = baseline\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    if previous_action == None:\n",
        "      action = np.random.randint(self._number_of_arms)\n",
        "    else:\n",
        "      #update values\n",
        "      self.t +=1\n",
        "      if self._baseline:\n",
        "        #track average\n",
        "        self.avg_r = self.r/self.t\n",
        "      else:\n",
        "        #do not track average\n",
        "        self.avg_r = 0\n",
        "      \n",
        "      self.r += reward\n",
        "      #update: For a=a_t: += alpha * (r - baseline)* 2 * (1/p - p/sum(p**2))), note: 2/(1/p - p/sum(p**2))\n",
        "      self.p[previous_action]+= self._step_size*(reward - self.avg_r)*2*(1/self.p[previous_action] - self.p[previous_action]/np.sum(np.power(self.p,2)))\n",
        "      #update: For a!= a_t: += alhpa * (r - baseline) * 2* ( 0/p - p/sum(p**2)) = -alpha*(r-baseline)*2*(p/sum(p**2))\n",
        "      self.p[0:previous_action-1] -= self._step_size*(reward - self.avg_r)*2*self.p[0:previous_action-1]/np.sum(np.power(self.p,2))\n",
        "      self.p[previous_action+1:self._number_of_arms]-= self._step_size*(reward - self.avg_r)*2*self.p[previous_action+1:self._number_of_arms]/np.sum(np.power(self.p,2))\n",
        "      #calc new pi\n",
        "      self.prob = np.power(self.p,2)/np.sum(np.power(self.p,2))\n",
        "      \n",
        "      # action = np.argmax(self.prob)\n",
        "      action = np.random.choice(self._number_of_arms, p = self.prob)\n",
        "        \n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    self.t = 0\n",
        "    self.r = 0\n",
        "    self.p = np.ones(self._number_of_arms)\n",
        "    self.prob = np.power(self.p,2)/np.sum(np.power(self.p,2))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZsPzCmDxAh"
      },
      "source": [
        "# B) Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQkk8sMxE0N4"
      },
      "source": [
        "**Run the cell below to train the agents and generate the plots for the first experiment.**\n",
        "\n",
        "Trains the agents on a Bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 1, and a reward on failure of 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06P4AfZw1GSH"
      },
      "source": [
        "## Experiment 1: Bernoulli bandit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loe_YN7Tv8yY"
      },
      "outputs": [],
      "source": [
        "%%capture experiment1\n",
        "\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "agents = [\n",
        "    Random(\n",
        "        \"random\",\n",
        "        number_of_arms),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0.1$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.1),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/t$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/\\sqrt{t}$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t**0.5),\n",
        "    UCB(\"UCB\",\n",
        "        number_of_arms,\n",
        "        bonus_multiplier=1/np.sqrt(2)),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=False),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE with baseline, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=True),\n",
        "]\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnbpeszVp04-"
      },
      "outputs": [],
      "source": [
        "experiment1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyaMm-sjz9a"
      },
      "source": [
        "## Q4 [4 pts total]\n",
        "(Answer inline in the markdown below each question, **within this text cell**.)\n",
        "\n",
        "**[2 pts]**\n",
        "For each algorithm in the plots above, explain whether or not we should be expected it to be good in general, in terms of total regret.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "In terms of total regret UCB, Reinforce with baseline, and $\\epsilon$-greedy with $\\epsilon$ = .1 and $1/\\sqrt{t}$ perform well.\n",
        "\n",
        "UCB does a good job of balancing exploitation and exploration. This optimization is the key to good results in terms of total regret. This is because when UCB is choosing random actions during exploration it takes into account the potential of random actions for being optimal. In some ways it can be similar to $\\epsilon$-greedy, however, the difference is that looking at the action selection formula for UCB: $A_{t}$ = argmax$_{t}[Q_{t} + c\\sqrt{ln(t)/N_{t}(a)]}$, the square root operates as a measure of the uncertainty of estimate's a value and c acts as how confident we are. So then each time a is selected the uncertainty decreases, and if another action is selected then uncertainty increases. \n",
        "\n",
        "\n",
        "$\\epsilon$-greedy when exploring chooses randomly. So it is less optimal than UCB but can still behave well depending on the value chosen for $\\epsilon$. Depending on the value chosen (this is expanded on more below in the next answer) the algorithm will either spend more time exploiting or exploring, this can result in some less than optimal results if the parameter is not chosen well.\n",
        "\n",
        "Reinforce similar to UCB results in an algorithm that balances exploitation and exploration well. However a preference is estimated instead of action values. And updateds are based on stochastic gradient ascent. If a baseline is used, the reward is compared with this at every timestep. If the reward is below baseline then the probability of picking this action is reduced, but if the reward is above it is increased. Non selected actions move in the opposite direction of the selected action update. Without this baseline for comparison, we cannot optimise in this way and instead if a reward is positive even if it is not the optimal action, if picked the preference will be updated. If $\\epsilon$-greedy is not chosen optimally Reinforce will often perform better in terms of total regret, but less well than UCB.\n",
        "\n",
        "**[2 pts]** Explain the relative ranking of the $\\epsilon$-greedy algorithms in this experiment.\n",
        "\n",
        "**Answer:** In order of increasing performance:\n",
        "1. $\\epsilon$ = 0 : This is the worst performing iteration of the $\\epsilon$- greedy algorithm. Since this agent does not explore at all, the first action that results in a  reward will continuously be selected irregardless of any change in the environment.\n",
        "2. $\\epsilon$ = 1/t : While the agent will explore early on, as t increases, exploration becomes less and less likely and the agent will behave more like $\\epsilon = 0$. Essentially while there is some exploration in the beginning, the time spent exploring decreases too quickly compared to the other algoritms.\n",
        "3. $\\epsilon$ = .1 : This agent will continously explore throughout training and will result in a more optimal learning.\n",
        "4. $\\epsilon$ = 1/$\\sqrt{t}$: The best performing value for $\\epsilon$ this algorithm, like the previous one explores throughout. However, instead of remaining constant, $\\epsilon$ slowly decreases at an optimum rate so that once a fairly clear picture of the environment is created the algorithm spends less and less time exploring."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YO5NDaPGDsp"
      },
      "source": [
        "## Experiment 2: reward = 0 on success, reward = -1 on failure.\n",
        "\n",
        "**Run the cell below to train the agents and generate the plots for the second experiment.**\n",
        "Reruns experiment 1 but on a different bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 0, and a reward on failure of -1.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "7cvJf4WzmJXK"
      },
      "outputs": [],
      "source": [
        "%%capture experiment2\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             success_reward=0., fail_reward=-1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5RXnvnFLOGa"
      },
      "outputs": [],
      "source": [
        "experiment2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GOe5RDsnj4J"
      },
      "source": [
        "## Q5 [2 pts]\n",
        "For each algorithm, note whether the performance changed significantly compared to the **experiment 1**, and explain why it did or did not.\n",
        "\n",
        "(Use at most two sentences per algorithm).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Performance changed significanty for epsilon greedy; this is because we penalize choosing the incorrect action so then the average of sampled rewards is less than (or equal to if the agent has chosen correctly in the beginning) zero. This essentially forces the algorithm to explore instead of always choosing the greedy action especially for $\\epsilon$ values that are less than optimal like $\\epsilon$ = 0 and $1/t$.\n",
        "\n",
        "UCB performs just as well as before. This is because the exploration and explortation trade off in this algorithm isn't much affected by the change in rewards. \n",
        "\n",
        "The Reinforce algorithm still to work fairly well, however the algorithm without baseline increases performance significantly and is comparable or even better to the algorithm with baseline. This is beause without a baseline term moderating the update, selecting the wrong action will result in negative rewards that will punish the incorrect choice greater than if there had been a baseline. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRDmw4nyFI73"
      },
      "source": [
        "## Run the following cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drtsr8Cc1OWl"
      },
      "source": [
        "## Experiment 3: Non-stationary bandit\n",
        " * Reward on `failure` changes from 0 to +2.\n",
        " * Reward on `success` remains at +1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4RseDt-MCkq"
      },
      "outputs": [],
      "source": [
        "%%capture experiment3\n",
        "\n",
        "number_of_arms = 3\n",
        "number_of_steps = 1984\n",
        "agents = [\n",
        "    Random(\n",
        "        \"random\",\n",
        "        number_of_arms),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0.1$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.1),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/\\sqrt{t}$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t**0.5),\n",
        "    UCB(\"UCB\",\n",
        "        number_of_arms,\n",
        "        bonus_multiplier=1/np.sqrt(2)),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE with baseline, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=True),\n",
        "\n",
        "]\n",
        "\n",
        "roving_bandit_class = partial(NonStationaryBandit, change_is_good=True)\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             bandit_class=roving_bandit_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sErI1V9h1ScE"
      },
      "source": [
        "## Experiment 4: Non-stationary bandit\n",
        " * Reward on `failure` changes from 0 to +1.\n",
        " * Reward on `success` changes from +1 to 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT_mZxCIAfg9"
      },
      "outputs": [],
      "source": [
        "%%capture experiment4\n",
        "\n",
        "number_of_arms = 3\n",
        "number_of_steps = 1984\n",
        "\n",
        "\n",
        "roving_bandit_class = partial(NonStationaryBandit, change_is_good=False)\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             bandit_class=roving_bandit_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aWjHNwbEsDJ"
      },
      "outputs": [],
      "source": [
        "experiment3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s703_VCICCTL"
      },
      "outputs": [],
      "source": [
        "experiment4.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x84zO7DK2_t"
      },
      "source": [
        "## Q6 [9 pts total]\n",
        "\n",
        "Observe the reward and regret curves above.  After 800 steps, the rewards change. In **experiment 3** `success` continues to yield a reward of +1, but `failure` changes from a reward of 0 to a reward of +2.  In **experiment 4**, `success` is now worth 0 and `failure` is worth +1.\n",
        "\n",
        "Below, we ask for explanations.  Answer each question briefly, using at most three sentences per question.\n",
        "\n",
        "**[2 pts]** In **experiment 3** explain the ranking in current regret after the change in rewards for all algorithms.\n",
        "\n",
        "> **Answer:** After the change in rewards, failure becomes more rewarding than success by the same amount that it was less successful previously. The algorithms that are most able to adapt to this change the quickest and minimize regret are Reinforce and $\\epsilon$-greedy for $\\epsilon = .1$. This is because Reinforce uses probability to select actions that return the lowest regret and can sense that change in the distribution based on feedback and ajust easily. Similarly since $\\epsilon = .1$ explores consistently throughout training, it will be more able to pick up on the change in rewards later on and adjust whereas $\\epsilon = 1/\\sqrt{t}$ and $1/t$ have reduced their epsilon this late in traning that they rarely explore and will need more time to adapt. For $\\epsilon = 0$ as explained before will not adapt once it chooses a successful action and therefore performs the worst.\n",
        "\n",
        "**[2 pts]** In **experiment 4** explain the ranking in current regret after the change in rewards for all algorithms.\n",
        "\n",
        "> **Answer:** \n",
        "\n",
        "**[2 pts]** Explain how and why the current-regret curve for UCB in **experiment 3** differs from the curve in **experiment 4**.\n",
        "\n",
        "> **Answer:**\n",
        "\n",
        "**[3 pts]** In general, if rewards can be non-stationary, and we don't know the exact nature of the non-stationarity, how could we modify UCB to perform better?   Be specific and concise.\n",
        "\n",
        "> **Answer:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGhirrkCChAZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "UCL RL assignment 2022, part I",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}